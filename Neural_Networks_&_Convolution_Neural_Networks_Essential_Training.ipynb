{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZyM6lAoR7UTG",
        "Fn0_22SDP87a",
        "HEeco1tBaLvY",
        "_KVxUhBiaOHW",
        "qomBM2NwFsxf",
        "VctGnC68ajYa",
        "4pDeU-eGMk2L",
        "6MUWeC1m8Fh-",
        "8f7kFf4xKqM3"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZfMCH3Jegv"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pJNvpjRT672w6Wp5sZJnC56oMhqmch2k?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGJ4gCA4KYE4"
      },
      "source": [
        "# Neural Networks & Convolution Neural Networks Essential Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap4L82SoIbWf"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the training data\n",
        "train_images = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_images = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "NKjRLjaISpbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing the Dataset"
      ],
      "metadata": {
        "id": "ZyM6lAoR7UTG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX5pwHNbBMrz"
      },
      "source": [
        "**Confirm type and dimensions of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqcWURO4Irbo"
      },
      "source": [
        "train_images.data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2SUNvKrI2RP"
      },
      "source": [
        "test_images.data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGuQVbA1JTPM"
      },
      "source": [
        "**How many instances do we have of each of the different classes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGGcK9OGI4Rl"
      },
      "source": [
        "np.unique(train_images.targets, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CwOVTMuMSfu"
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v83v_MQrJGPR"
      },
      "source": [
        "# Get 16 images at random\n",
        "import random\n",
        "random.seed(1)\n",
        "train_idx = list(range(len(train_images)))\n",
        "sampler = random.sample(train_idx, 16)\n",
        "sampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae5I_i2NLMYg"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i, idx in enumerate(sampler):\n",
        "    image, label = train_images[idx]\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image.squeeze(), cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[label])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oijYUBt__lh"
      },
      "source": [
        "**What does one image look like?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_y4upVPIe8_"
      },
      "source": [
        "random_image_id = 1\n",
        "image, label = train_images[random_image_id]\n",
        "plt.figure()\n",
        "plt.imshow(image.squeeze(), cmap=plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21_hgve7-Dnr"
      },
      "source": [
        "class_names[label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[random_image_id][0].shape"
      ],
      "metadata": {
        "id": "Lsj_gvUUGcOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw-zJYAf_Bjl"
      },
      "source": [
        "train_images[random_image_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn0_22SDP87a"
      },
      "source": [
        "## Defining the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "102iKgH9_pnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv-cMB5PAERF"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(28 * 28, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(64, 10)\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define the layers of the network\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # No activation on the final layer for raw logits\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model and move it to the desired device\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "GGVYaFGgTk4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "HEeco1tBaLvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See course video"
      ],
      "metadata": {
        "id": "UXK3UqKOaR1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "_KVxUhBiaOHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See course video"
      ],
      "metadata": {
        "id": "M8NAdlstaXTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "qomBM2NwFsxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YoSlu1GwF86S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "VctGnC68ajYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "wY9LDeRNUr36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before we train a Neural Network"
      ],
      "metadata": {
        "id": "4pDeU-eGMk2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define the layers of the network\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "0DjUNPPCQlyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the training data\n",
        "train_images = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_images = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=False)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "cLZmJ3b2unKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_id = 12 # Pick an image id between 0 and 9999 inclusive\n",
        "\n",
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Get a single image and its corresponding label\n",
        "img = images[image_id].to(device)\n",
        "true_label = labels[image_id].to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    logits = model(img.unsqueeze(0)) # We extract a single test image. Get raw prediction scores (logits)\n",
        "\n",
        "ps = torch.nn.functional.softmax(logits, dim=1).cpu() #convert those to probabilities using the exponential function, and identify the predicted class.\n",
        "predictions = ps.numpy()[0]\n",
        "predicted_label = np.argmax(predictions)\n",
        "\n",
        "def plot_image(predictions_array, true_label, img):\n",
        "  '''The visualization functions help interpret model behavior by showing not just what it predicted, but how confident it was about each possible class.'''\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img.cpu().numpy().squeeze(), cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(f\"{class_names[predicted_label]} {100*np.max(predictions_array):2.0f}% ({class_names[true_label]})\", color=color)\n",
        "\n",
        "def plot_value_array(predictions_array, true_label):\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('green')\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(predictions, true_label.item(), img)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(predictions,  true_label.item())\n",
        "_ = plt.xticks(range(10), class_names, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bBcVhlyuTjTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Neural Network"
      ],
      "metadata": {
        "id": "6MUWeC1m8Fh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define the layers of the network\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # No activation on the final layer for raw logits\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model and move it to the desired device\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "E04WrXy1r74R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the training data\n",
        "train_images = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_images = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=False)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "f5l2jnpIr74V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "huTOmUM9r_Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "kUHjq-ltsMby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rb8bR7DMcGE"
      },
      "source": [
        "model.train()\n",
        "\n",
        "epochs = epochs\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images) # 1. Forward Pass\n",
        "        loss = loss_function(output, labels) #2.  Calculate Loss\n",
        "        loss.backward() # 3. Backward Pass\n",
        "        optimizer.step() # 4. Optimizer Step\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(train_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NZjub5VP1Z"
      },
      "source": [
        "image_id = 13 # Pick an image id between 0 and 9999 inclusive\n",
        "\n",
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Get a single image and its corresponding label\n",
        "img = images[image_id].to(device)\n",
        "true_label = labels[image_id].to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad(): #This tells PyTorch we are only doing inference (predicting), not training, so it doesn't need to keep track of gradients.\n",
        "                      #This makes the process faster and uses less memory.\n",
        "    logits = model(img.unsqueeze(0)) # We extract a single test image, pass it through the model to get raw prediction scores (logits)\n",
        "\n",
        "ps = torch.nn.functional.softmax(logits, dim=1).cpu() #convert those to probabilities using the exponential function, and identify the predicted class.\n",
        "predictions = ps.numpy()[0]\n",
        "predicted_label = np.argmax(predictions) #Finally, np.argmax() finds the index of the class with the highest probability, which becomes our model's final prediction.\n",
        "\n",
        "#The visualization functions help interpret model behavior by showing not just what it predicted, but how confident it was about each possible class.\n",
        "def plot_image(predictions_array, true_label, img):\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img.cpu().numpy().squeeze(), cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(f\"{class_names[predicted_label]} {100*np.max(predictions_array):2.0f}% ({class_names[true_label]})\", color=color)\n",
        "\n",
        "def plot_value_array(predictions_array, true_label):\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('green')\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(predictions, true_label.item(), img)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(predictions,  true_label.item())\n",
        "_ = plt.xticks(range(10), class_names, rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKSnhhJWB70o"
      },
      "source": [
        "## Testing the Neural Network Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=False)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "test(test_loader, model, loss_function)"
      ],
      "metadata": {
        "id": "-Ix-1LSYLf5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution Neural Networks"
      ],
      "metadata": {
        "id": "wdXEz7_MF87w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7CEZn3qgFMw"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # Input (1, 28, 28)\n",
        "        # First convolutional layer: takes 1 input channel (grayscale), produces 6 output feature maps using 5x5 filters.\n",
        "        # padding=2 adds 2 pixels of zeros around the image edges, keeping output size at 28x28.\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5, padding=2) # Output: (6, 28, 28)\n",
        "        # Max pooling layer with 2x2 window and stride of 2.\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2) # Output: (6, 14, 14)\n",
        "        # Feature maps (lenet has 16 here)\n",
        "        # Size of the kernel 5x5\n",
        "        # No padding spatial size reduces from 14x14 to 10x10.\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 5) # Output: (16, 10, 10)\n",
        "        # After a second pooling, the size will be (16, 5, 5)\n",
        "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5) # Flatten the 3D output (16x5x5) into a 1D vector to feed into the fully connected layers.\n",
        "        #The -1 tells PyTorch to automatically calculate the batch size.\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = LeNet().to(device)"
      ],
      "metadata": {
        "id": "l23PFsNfEkGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the training data\n",
        "train_images = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_images = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=False)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "8xjfhGwGxvdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "fyqFXptZGj1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "epochs = epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images) # 1. Forward Pass\n",
        "        loss = loss_function(output, labels) # 2. Calculate Loss\n",
        "        loss.backward() # 3. Backward Pass\n",
        "        optimizer.step() # 4. Optimizer Step\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "wuH1u8oDG7LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "test(test_loader, model, loss_function)"
      ],
      "metadata": {
        "id": "Ch0kcbyFG7vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10 Classification"
      ],
      "metadata": {
        "id": "pgiB1a0fKqM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "-dRChVw_KqM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images\n",
        "    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                         std=[0.2471, 0.2435, 0.2616])\n",
        "])\n",
        "\n",
        "## ðŸ’¾ Download and load the training and testing data\n",
        "train_images = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_images = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "## ðŸ“¦ Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=64, shuffle=False)\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "o-GZbTaYKqM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09124c0-1cd9-4be5-a012-4e518fa13aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:06<00:00, 26.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "8f7kFf4xKqM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(32 * 32 * 3, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 128)\n",
        "        self.fc3 = torch.nn.Linear(128, 10)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "7ohnNkbhKqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "dFHib8DkKqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "epochs = epochs\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)  # 1. Forward Pass\n",
        "        loss = loss_function(output, labels) # 2. Calculate Loss\n",
        "        loss.backward()  # 3. Backward Pass\n",
        "        optimizer.step()  # 4. Optimizer Step\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "wSMi_qBJKqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "test(test_loader, model, loss_function)"
      ],
      "metadata": {
        "id": "RwvLYM0gKqM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_id = 13 # Pick an image id between 0 and 9999 inclusive\n",
        "\n",
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# If you don't de-normalize, the colors might look wrong.\n",
        "img = images[image_id].to(device)\n",
        "true_label = labels[image_id].to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad(): # Inference mode\n",
        "    logits = model(img.unsqueeze(0))\n",
        "\n",
        "ps = torch.nn.functional.softmax(logits, dim=1).cpu()\n",
        "predictions = ps.numpy()[0]\n",
        "predicted_label = np.argmax(predictions)\n",
        "\n",
        "def plot_image(predictions_array, true_label, img):\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    img_display = np.clip(img.cpu().numpy().transpose((1, 2, 0)), 0, 1)\n",
        "\n",
        "    plt.imshow(img_display)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(f\"{class_names[predicted_label]} {100*np.max(predictions_array):2.0f}% ({class_names[true_label]})\", color=color)\n",
        "\n",
        "def plot_value_array(predictions_array, true_label):\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(10))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('green')\n",
        "\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(predictions, true_label.item(), img)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(predictions, true_label.item())\n",
        "_ = plt.xticks(range(10), class_names, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gYjybcJKqM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolution Neural Network"
      ],
      "metadata": {
        "id": "mXlXdCWTKqM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional block\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
        "        self.pool1 = torch.nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(64)\n",
        "        self.conv4 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = torch.nn.BatchNorm2d(64)\n",
        "        self.pool2 = torch.nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
        "\n",
        "        # Third convolutional block\n",
        "        self.conv5 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5 = torch.nn.BatchNorm2d(128)\n",
        "        self.conv6 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn6 = torch.nn.BatchNorm2d(128)\n",
        "        self.pool3 = torch.nn.MaxPool2d(2, 2)  # 8x8 -> 4x4\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(128 * 4 * 4, 256)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "        self.fc2 = torch.nn.Linear(256, 128)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.fc3 = torch.nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Fully connected\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "lh8YNKAbKqM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7c8832-761a-444d-a28e-3fe4a060de77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "dCg35lr4KqM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "epochs = epochs\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images) # 1. Forward Pass\n",
        "        loss = loss_function(output, labels)  # 2. Calculate Loss\n",
        "        loss.backward()  # 3. Backward Pass\n",
        "        optimizer.step() # 4. Optimizer Step\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "-g3hX-WqKqM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5379f24-803e-4635-9f62-72348efe48e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training loss: 1.6988891953092706\n",
            "Epoch 2 - Training loss: 1.2970508093876607\n",
            "Epoch 3 - Training loss: 1.1134438138178853\n",
            "Epoch 4 - Training loss: 0.9957013494523285\n",
            "Epoch 5 - Training loss: 0.9204781759349282\n",
            "Epoch 6 - Training loss: 0.8590260141188532\n",
            "Epoch 7 - Training loss: 0.8058946384950672\n",
            "Epoch 8 - Training loss: 0.7581985342456862\n",
            "Epoch 9 - Training loss: 0.7240831308886219\n",
            "Epoch 10 - Training loss: 0.6911245312379755\n",
            "Epoch 11 - Training loss: 0.657084466551271\n",
            "Epoch 12 - Training loss: 0.6329069356707966\n",
            "Epoch 13 - Training loss: 0.6121768147667961\n",
            "Epoch 14 - Training loss: 0.5960293908024688\n",
            "Epoch 15 - Training loss: 0.5652160652915535\n",
            "Epoch 16 - Training loss: 0.5427116168581921\n",
            "Epoch 17 - Training loss: 0.5269002633174057\n",
            "Epoch 18 - Training loss: 0.5076768992425841\n",
            "Epoch 19 - Training loss: 0.4908823175998905\n",
            "Epoch 20 - Training loss: 0.4770305152896725\n",
            "Epoch 21 - Training loss: 0.4678051371860992\n",
            "Epoch 22 - Training loss: 0.4465388713780876\n",
            "Epoch 23 - Training loss: 0.4372641122935678\n",
            "Epoch 24 - Training loss: 0.42369997139324617\n",
            "Epoch 25 - Training loss: 0.4158511134936377\n",
            "Epoch 26 - Training loss: 0.40908402012056094\n",
            "Epoch 27 - Training loss: 0.39532437173606794\n",
            "Epoch 28 - Training loss: 0.3771208497455053\n",
            "Epoch 29 - Training loss: 0.3757115314378763\n",
            "Epoch 30 - Training loss: 0.3631155577099994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "test(test_loader, model, loss_function)"
      ],
      "metadata": {
        "id": "HOWYRO5NKqM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1070606-7bd7-4510-aa34-15e585453a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.423681 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_id = 15 # Pick an image id between 0 and 9999 inclusive\n",
        "\n",
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "img = images[image_id].to(device)\n",
        "true_label = labels[image_id].to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad(): # Inference mode\n",
        "    logits = model(img.unsqueeze(0))\n",
        "\n",
        "ps = torch.nn.functional.softmax(logits, dim=1).cpu()\n",
        "predictions = ps.numpy()[0]\n",
        "predicted_label = np.argmax(predictions)\n",
        "\n",
        "def plot_image(predictions_array, true_label, img):\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    img_display = np.clip(img.cpu().numpy().transpose((1, 2, 0)), 0, 1)\n",
        "\n",
        "    plt.imshow(img_display)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(f\"{class_names[predicted_label]} {100*np.max(predictions_array):2.0f}% ({class_names[true_label]})\", color=color)\n",
        "\n",
        "def plot_value_array(predictions_array, true_label):\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(10))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('green')\n",
        "\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(predictions, true_label.item(), img)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(predictions, true_label.item())\n",
        "_ = plt.xticks(range(10), class_names, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lcnltIo8KqM5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "01fe2ea6-87f8-455d-ab89-45725473519a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "S.D.G."
      ],
      "metadata": {
        "id": "N7oc7sGN_xrB"
      }
    }
  ]
}